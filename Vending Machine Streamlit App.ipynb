{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFtJ9N54N+jPVeH+DYv8lf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Overview\n","We are creating a Streamlit web app to run on Raspberry Pi. The app should do object detection using our YOLOv8 model in ONNX format.\n","\n","Code is based off https://github.com/JackDance/YOLOv8-streamlit-app\n","\n","# Download Model"],"metadata":{"id":"ZCY_ND6m5GGI"}},{"cell_type":"code","source":["!gdown --id 1hvpitHtXvmUZhLa7ebfTxrfXn186VrF3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd2_1X-m5CHz","executionInfo":{"status":"ok","timestamp":1720745375133,"user_tz":420,"elapsed":5888,"user":{"displayName":"Aadi Adgaonkar","userId":"09049555627763488271"}},"outputId":"565ad72e-f2d3-45b8-a7c1-5016428f8aa7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From: https://drive.google.com/uc?id=1hvpitHtXvmUZhLa7ebfTxrfXn186VrF3\n","To: /content/Vending-YOLOv8n.onnx\n","100% 12.2M/12.2M [00:00<00:00, 27.2MB/s]\n"]}]},{"cell_type":"markdown","source":["# Packages"],"metadata":{"id":"dyyZRt9097Lz"}},{"cell_type":"code","source":["!pip install ultralytics -q\n","!pip install streamlit -q\n","!pip install onnxruntime -q\n","!pip install onnx -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4qa2pQfs-J8r","executionInfo":{"status":"ok","timestamp":1720745490211,"user_tz":420,"elapsed":115086,"user":{"displayName":"Aadi Adgaonkar","userId":"09049555627763488271"}},"outputId":"8c891288-ee82-4ed4-d833-79b070a0640c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.1/800.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["# Streamlit App"],"metadata":{"id":"WrYBMqaJ5OgM"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_Ik6ZkS4_AS","executionInfo":{"status":"ok","timestamp":1720745557469,"user_tz":420,"elapsed":508,"user":{"displayName":"Aadi Adgaonkar","userId":"09049555627763488271"}},"outputId":"710c2452-863e-4620-d84e-3d7f9dc4316a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}],"source":["%%writefile app.py\n","\n","from ultralytics import YOLO\n","import streamlit as st\n","import cv2\n","from PIL import Image\n","import tempfile\n","\n","def _display_detected_frames(conf, model, st_frame, image):\n","    \"\"\"\n","    Display the detected objects on a video frame using the YOLOv8 model.\n","    :param conf (float): Confidence threshold for object detection.\n","    :param model (YOLOv8): An instance of the `YOLOv8` class containing the YOLOv8 model.\n","    :param st_frame (Streamlit object): A Streamlit object to display the detected video.\n","    :param image (numpy array): A numpy array representing the video frame.\n","    :return: None\n","    \"\"\"\n","    # Resize the image to a standard size\n","    image = cv2.resize(image, (720, int(720 * (9 / 16))))\n","\n","    # Predict the objects in the image using YOLOv8 model\n","    res = model.predict(image, conf=conf)\n","\n","    # Plot the detected objects on the video frame\n","    res_plotted = res[0].plot()\n","    st_frame.image(res_plotted,\n","                   caption='Detected Video',\n","                   channels=\"BGR\",\n","                   use_column_width=True\n","                   )\n","    num_brown = 0\n","    num_red = 0\n","    for pred in res[0].boxes.cls:\n","        if pred == 0:\n","            num_brown += 1\n","        elif pred == 1:\n","            num_red += 1\n","\n","    st.subheader(f\"Number of brown: {num_brown}\")\n","    st.subheader(f\"Number of red: {num_red}\")\n","\n","\n","@st.cache_resource\n","def load_model(model_path):\n","    \"\"\"\n","    Loads a YOLO object detection model from the specified model_path.\n","\n","    Parameters:\n","        model_path (str): The path to the YOLO model file.\n","\n","    Returns:\n","        A YOLO object detection model.\n","    \"\"\"\n","    model = YOLO(model_path)\n","    return model\n","\n","\n","def infer_uploaded_image(conf, model):\n","    \"\"\"\n","    Execute inference for uploaded image\n","    :param conf: Confidence of YOLOv8 model\n","    :param model: An instance of the `YOLOv8` class containing the YOLOv8 model.\n","    :return: None\n","    \"\"\"\n","    source_img = st.sidebar.file_uploader(\n","        label=\"Choose an image...\",\n","        type=(\"jpg\", \"jpeg\", \"png\", 'bmp', 'webp')\n","    )\n","\n","    col1, col2 = st.columns(2)\n","\n","    with col1:\n","        if source_img:\n","            uploaded_image = Image.open(source_img)\n","            # adding the uploaded image to the page with caption\n","            st.image(\n","                image=source_img,\n","                caption=\"Uploaded Image\",\n","                use_column_width=True\n","            )\n","\n","    if source_img:\n","        if st.button(\"Execution\"):\n","            with st.spinner(\"Running...\"):\n","                res = model.predict(uploaded_image,\n","                                    conf=conf)\n","                boxes = res[0].boxes\n","                res_plotted = res[0].plot()[:, :, ::-1]\n","\n","                num_brown = 0\n","                num_red = 0\n","                for pred in res[0].boxes.cls:\n","                    if pred == 0:\n","                        num_brown += 1\n","                    elif pred == 1:\n","                        num_red += 1\n","\n","                with col2:\n","                    st.image(res_plotted,\n","                             caption=\"Detected Image\",\n","                             use_column_width=True)\n","                    st.subheader(f\"Number of brown: {num_brown}\")\n","                    st.subheader(f\"Number of red: {num_red}\")\n","\n","\n","def infer_uploaded_video(conf, model):\n","    \"\"\"\n","    Execute inference for uploaded video\n","    :param conf: Confidence of YOLOv8 model\n","    :param model: An instance of the `YOLOv8` class containing the YOLOv8 model.\n","    :return: None\n","    \"\"\"\n","    source_video = st.sidebar.file_uploader(\n","        label=\"Choose a video...\"\n","    )\n","\n","    if source_video:\n","        st.video(source_video)\n","\n","    if source_video:\n","        if st.button(\"Execution\"):\n","            with st.spinner(\"Running...\"):\n","                try:\n","                    tfile = tempfile.NamedTemporaryFile()\n","                    tfile.write(source_video.read())\n","                    vid_cap = cv2.VideoCapture(\n","                        tfile.name)\n","                    st_frame = st.empty()\n","                    while (vid_cap.isOpened()):\n","                        success, image = vid_cap.read()\n","                        if success:\n","                            _display_detected_frames(conf,\n","                                                     model,\n","                                                     st_frame,\n","                                                     image\n","                                                     )\n","                        else:\n","                            vid_cap.release()\n","                            break\n","                except Exception as e:\n","                    st.error(f\"Error loading video: {e}\")\n","\n","\n","def infer_uploaded_webcam(conf, model):\n","    \"\"\"\n","    Execute inference for webcam.\n","    :param conf: Confidence of YOLOv8 model\n","    :param model: An instance of the `YOLOv8` class containing the YOLOv8 model.\n","    :return: None\n","    \"\"\"\n","    try:\n","        flag = st.button(\n","            label=\"Stop running\"\n","        )\n","        vid_cap = cv2.VideoCapture(0)  # local camera\n","        st_frame = st.empty()\n","        while not flag:\n","            success, image = vid_cap.read()\n","            if success:\n","                _display_detected_frames(\n","                    conf,\n","                    model,\n","                    st_frame,\n","                    image\n","                )\n","            else:\n","                vid_cap.release()\n","                break\n","    except Exception as e:\n","        st.error(f\"Error loading video: {str(e)}\")\n","\n","def main():\n","    #!/usr/bin/env python\n","    # -*- coding: utf-8 -*-\n","    from pathlib import Path\n","    from PIL import Image\n","    import streamlit as st\n","\n","\n","    # setting page layout\n","    st.set_page_config(\n","        page_title=\"Interactive Interface for YOLOv8\",\n","        page_icon=\"🤖\",\n","        layout=\"wide\",\n","        initial_sidebar_state=\"expanded\"\n","        )\n","\n","    # main page heading\n","    st.title(\"Interactive Interface for YOLOv8\")\n","\n","    # load pretrained DL model\n","    model = load_model('/content/Vending-YOLOv8n.onnx')\n","\n","    # image/video options\n","    st.sidebar.header(\"Image/Video Config\")\n","    source_selectbox = st.sidebar.selectbox(\n","        \"Select Source\",\n","        [\"Image\", \"Video\", \"Webcam\"]\n","    )\n","\n","    source_img = None\n","    confidence = 0.5\n","    if source_selectbox == \"Image\": # Image\n","        infer_uploaded_image(confidence, model)\n","    elif source_selectbox == \"Video\": # Video\n","        infer_uploaded_video(confidence, model)\n","    elif source_selectbox == \"Webcam\": # Webcam\n","        infer_uploaded_webcam(confidence, model)\n","    else:\n","        st.error(\"Currently only 'Image' and 'Video' source are implemented\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","source":["!curl ipecho.net/plain\n","# THIS IS THE PASSWORD FOR LOCALTUNNEL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEFYqt_R7NwN","executionInfo":{"status":"ok","timestamp":1720745500445,"user_tz":420,"elapsed":683,"user":{"displayName":"Aadi Adgaonkar","userId":"09049555627763488271"}},"outputId":"c319a708-81e3-4959-8d32-8486391450dc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["34.91.23.244"]}]},{"cell_type":"code","source":["!streamlit run app.py & npx localtunnel --port 8501"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsiH335b6Sef","outputId":"a3a6ca94-91f5-4fae-e959-f0f9beab1cd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n","\u001b[0m\n","\u001b[0m\n","\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n","\u001b[0m\n","\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n","\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n","\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.91.23.244:8501\u001b[0m\n","\u001b[0m\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.347s\n","your url is: https://afraid-poets-obey.loca.lt\n","WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n","Loading /content/Vending-YOLOv8n.onnx for ONNX Runtime inference...\n","\n","0: 640x640 10 Browns, 11 Reds, 240.9ms\n","Speed: 6.4ms preprocess, 240.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 356.2ms\n","Speed: 6.8ms preprocess, 356.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 295.9ms\n","Speed: 4.9ms preprocess, 295.9ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 279.9ms\n","Speed: 5.0ms preprocess, 279.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 207.1ms\n","Speed: 3.9ms preprocess, 207.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 9 Browns, 243.4ms\n","Speed: 5.7ms preprocess, 243.4ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 293.0ms\n","Speed: 4.7ms preprocess, 293.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 315.5ms\n","Speed: 4.9ms preprocess, 315.5ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 Brown, 2 Reds, 187.8ms\n","Speed: 4.0ms preprocess, 187.8ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 186.3ms\n","Speed: 3.8ms preprocess, 186.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 332.2ms\n","Speed: 10.3ms preprocess, 332.2ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 5 Reds, 192.7ms\n","Speed: 3.8ms preprocess, 192.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 187.1ms\n","Speed: 4.4ms preprocess, 187.1ms inference, 4.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 283.7ms\n","Speed: 4.7ms preprocess, 283.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 9 Browns, 183.5ms\n","Speed: 3.8ms preprocess, 183.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 10 Browns, 10 Reds, 181.8ms\n","Speed: 4.1ms preprocess, 181.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"N3uzuVoaMp5J"},"execution_count":null,"outputs":[]}]}